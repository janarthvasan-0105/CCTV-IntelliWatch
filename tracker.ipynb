{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e8e17e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Neuromouse\\cuda\\Lib\\site-packages\\torchreid\\reid\\metrics\\rank.py:11: UserWarning: Cython evaluation (very fast so highly recommended) is unavailable, now use python evaluation.\n",
      "  warnings.warn(\n",
      "d:\\Neuromouse\\cuda\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/attr_value.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "d:\\Neuromouse\\cuda\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "d:\\Neuromouse\\cuda\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/resource_handle.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "d:\\Neuromouse\\cuda\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_shape.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "d:\\Neuromouse\\cuda\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/types.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "d:\\Neuromouse\\cuda\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/full_type.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "d:\\Neuromouse\\cuda\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/function.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "d:\\Neuromouse\\cuda\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/node_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "d:\\Neuromouse\\cuda\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/op_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "d:\\Neuromouse\\cuda\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "d:\\Neuromouse\\cuda\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph_debug_info.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "d:\\Neuromouse\\cuda\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/versions.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "d:\\Neuromouse\\cuda\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "d:\\Neuromouse\\cuda\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at xla/tsl/protobuf/coordination_config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "d:\\Neuromouse\\cuda\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/cost_graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "d:\\Neuromouse\\cuda\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/step_stats.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "d:\\Neuromouse\\cuda\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/allocation_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "d:\\Neuromouse\\cuda\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "d:\\Neuromouse\\cuda\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/cluster.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "d:\\Neuromouse\\cuda\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/debug.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import datetime\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "import torchreid\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c72eadf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_model = YOLO(\"yolov8n.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d15bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded imagenet pretrained weights from \"C:\\Users\\janar/.cache\\torch\\checkpoints\\osnet_x0_25_imagenet.pth\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OSNet(\n",
       "  (conv1): ConvLayer(\n",
       "    (conv): Conv2d(3, 16, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (conv2): Sequential(\n",
       "    (0): OSBlock(\n",
       "      (conv1): Conv1x1(\n",
       "        (conv): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2a): LightConv3x3(\n",
       "        (conv1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
       "        (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2b): Sequential(\n",
       "        (0): LightConv3x3(\n",
       "          (conv1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
       "          (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): LightConv3x3(\n",
       "          (conv1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
       "          (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (conv2c): Sequential(\n",
       "        (0): LightConv3x3(\n",
       "          (conv1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
       "          (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): LightConv3x3(\n",
       "          (conv1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
       "          (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): LightConv3x3(\n",
       "          (conv1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
       "          (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (conv2d): Sequential(\n",
       "        (0): LightConv3x3(\n",
       "          (conv1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
       "          (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): LightConv3x3(\n",
       "          (conv1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
       "          (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): LightConv3x3(\n",
       "          (conv1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
       "          (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): LightConv3x3(\n",
       "          (conv1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
       "          (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (gate): ChannelGate(\n",
       "        (global_avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "        (fc1): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (fc2): Conv2d(1, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (gate_activation): Sigmoid()\n",
       "      )\n",
       "      (conv3): Conv1x1Linear(\n",
       "        (conv): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (downsample): Conv1x1Linear(\n",
       "        (conv): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): OSBlock(\n",
       "      (conv1): Conv1x1(\n",
       "        (conv): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2a): LightConv3x3(\n",
       "        (conv1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
       "        (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2b): Sequential(\n",
       "        (0): LightConv3x3(\n",
       "          (conv1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
       "          (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): LightConv3x3(\n",
       "          (conv1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
       "          (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (conv2c): Sequential(\n",
       "        (0): LightConv3x3(\n",
       "          (conv1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
       "          (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): LightConv3x3(\n",
       "          (conv1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
       "          (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): LightConv3x3(\n",
       "          (conv1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
       "          (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (conv2d): Sequential(\n",
       "        (0): LightConv3x3(\n",
       "          (conv1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
       "          (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): LightConv3x3(\n",
       "          (conv1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
       "          (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): LightConv3x3(\n",
       "          (conv1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
       "          (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): LightConv3x3(\n",
       "          (conv1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
       "          (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (gate): ChannelGate(\n",
       "        (global_avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "        (fc1): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (fc2): Conv2d(1, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (gate_activation): Sigmoid()\n",
       "      )\n",
       "      (conv3): Conv1x1Linear(\n",
       "        (conv): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Conv1x1(\n",
       "        (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    )\n",
       "  )\n",
       "  (conv3): Sequential(\n",
       "    (0): OSBlock(\n",
       "      (conv1): Conv1x1(\n",
       "        (conv): Conv2d(64, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2a): LightConv3x3(\n",
       "        (conv1): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (conv2): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
       "        (bn): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2b): Sequential(\n",
       "        (0): LightConv3x3(\n",
       "          (conv1): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (conv2): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
       "          (bn): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): LightConv3x3(\n",
       "          (conv1): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (conv2): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
       "          (bn): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (conv2c): Sequential(\n",
       "        (0): LightConv3x3(\n",
       "          (conv1): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (conv2): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
       "          (bn): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): LightConv3x3(\n",
       "          (conv1): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (conv2): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
       "          (bn): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): LightConv3x3(\n",
       "          (conv1): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (conv2): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
       "          (bn): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (conv2d): Sequential(\n",
       "        (0): LightConv3x3(\n",
       "          (conv1): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (conv2): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
       "          (bn): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): LightConv3x3(\n",
       "          (conv1): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (conv2): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
       "          (bn): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): LightConv3x3(\n",
       "          (conv1): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (conv2): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
       "          (bn): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): LightConv3x3(\n",
       "          (conv1): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (conv2): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
       "          (bn): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (gate): ChannelGate(\n",
       "        (global_avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "        (fc1): Conv2d(24, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (fc2): Conv2d(1, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (gate_activation): Sigmoid()\n",
       "      )\n",
       "      (conv3): Conv1x1Linear(\n",
       "        (conv): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (downsample): Conv1x1Linear(\n",
       "        (conv): Conv2d(64, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): OSBlock(\n",
       "      (conv1): Conv1x1(\n",
       "        (conv): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2a): LightConv3x3(\n",
       "        (conv1): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (conv2): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
       "        (bn): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2b): Sequential(\n",
       "        (0): LightConv3x3(\n",
       "          (conv1): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (conv2): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
       "          (bn): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): LightConv3x3(\n",
       "          (conv1): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (conv2): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
       "          (bn): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (conv2c): Sequential(\n",
       "        (0): LightConv3x3(\n",
       "          (conv1): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (conv2): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
       "          (bn): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): LightConv3x3(\n",
       "          (conv1): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (conv2): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
       "          (bn): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): LightConv3x3(\n",
       "          (conv1): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (conv2): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
       "          (bn): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (conv2d): Sequential(\n",
       "        (0): LightConv3x3(\n",
       "          (conv1): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (conv2): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
       "          (bn): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): LightConv3x3(\n",
       "          (conv1): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (conv2): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
       "          (bn): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): LightConv3x3(\n",
       "          (conv1): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (conv2): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
       "          (bn): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): LightConv3x3(\n",
       "          (conv1): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (conv2): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
       "          (bn): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (gate): ChannelGate(\n",
       "        (global_avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "        (fc1): Conv2d(24, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (fc2): Conv2d(1, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (gate_activation): Sigmoid()\n",
       "      )\n",
       "      (conv3): Conv1x1Linear(\n",
       "        (conv): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Conv1x1(\n",
       "        (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    )\n",
       "  )\n",
       "  (conv4): Sequential(\n",
       "    (0): OSBlock(\n",
       "      (conv1): Conv1x1(\n",
       "        (conv): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2a): LightConv3x3(\n",
       "        (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2b): Sequential(\n",
       "        (0): LightConv3x3(\n",
       "          (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): LightConv3x3(\n",
       "          (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (conv2c): Sequential(\n",
       "        (0): LightConv3x3(\n",
       "          (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): LightConv3x3(\n",
       "          (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): LightConv3x3(\n",
       "          (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (conv2d): Sequential(\n",
       "        (0): LightConv3x3(\n",
       "          (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): LightConv3x3(\n",
       "          (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): LightConv3x3(\n",
       "          (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): LightConv3x3(\n",
       "          (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (gate): ChannelGate(\n",
       "        (global_avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "        (fc1): Conv2d(32, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (fc2): Conv2d(2, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (gate_activation): Sigmoid()\n",
       "      )\n",
       "      (conv3): Conv1x1Linear(\n",
       "        (conv): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (downsample): Conv1x1Linear(\n",
       "        (conv): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): OSBlock(\n",
       "      (conv1): Conv1x1(\n",
       "        (conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2a): LightConv3x3(\n",
       "        (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2b): Sequential(\n",
       "        (0): LightConv3x3(\n",
       "          (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): LightConv3x3(\n",
       "          (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (conv2c): Sequential(\n",
       "        (0): LightConv3x3(\n",
       "          (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): LightConv3x3(\n",
       "          (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): LightConv3x3(\n",
       "          (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (conv2d): Sequential(\n",
       "        (0): LightConv3x3(\n",
       "          (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): LightConv3x3(\n",
       "          (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): LightConv3x3(\n",
       "          (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): LightConv3x3(\n",
       "          (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (gate): ChannelGate(\n",
       "        (global_avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "        (fc1): Conv2d(32, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (fc2): Conv2d(2, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (gate_activation): Sigmoid()\n",
       "      )\n",
       "      (conv3): Conv1x1Linear(\n",
       "        (conv): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv5): Conv1x1(\n",
       "    (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (global_avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "osnet_model = torchreid.models.build_model(\n",
    "    name='osnet_x0_25', \n",
    "    num_classes=1000, \n",
    "    pretrained=True\n",
    ")\n",
    "osnet_model.eval() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59f8ab38",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((256, 128)), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "391195bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Neuromouse\\cuda\\Lib\\site-packages\\deep_sort_realtime\\embedder\\embedder_pytorch.py:6: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "tracker = DeepSort(\n",
    "    max_age=200, \n",
    "    n_init=5, \n",
    "    nms_max_overlap=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9c3d595",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \"D:/cctv/16.avi\"\n",
    "video_cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "frame_width = int(video_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(video_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = video_cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "out = cv2.VideoWriter(\"output_person_tracking_7_suspicious.mp4\",\n",
    "                      cv2.VideoWriter_fourcc(*'mp4v'),\n",
    "                      fps, (frame_width, frame_height))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d0a9a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIDENCE_THRESHOLD = 0.5\n",
    "GREEN = (0, 255, 0)\n",
    "WHITE = (255, 255, 255)\n",
    "RED = (0, 0, 255)\n",
    "track_embedding_history = {}  # store embeddings for re-ID\n",
    "last_seen = {}                # last frame a track was seen\n",
    "REIDENTIFY_DELAY_FRAMES = 30  # delay before re-ID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e51c8d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "track_positions = {}  # store history of (x_center, y_center) per track\n",
    "SUSPICIOUS_TIME_FRAMES = int(fps * 5)  # e.g., 5 seconds\n",
    "STILLNESS_THRESHOLD = 10               # max movement in pixels to be considered \"still\"\n",
    "\n",
    "def is_suspicious(pos_history):\n",
    "    if len(pos_history) < SUSPICIOUS_TIME_FRAMES:\n",
    "        return False\n",
    "    x_coords, y_coords = zip(*pos_history)\n",
    "    if max(x_coords) - min(x_coords) < STILLNESS_THRESHOLD and \\\n",
    "       max(y_coords) - min(y_coords) < STILLNESS_THRESHOLD:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d16068f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 21 persons, 2 suitcases, 122.2ms\n",
      "Speed: 3.8ms preprocess, 122.2ms inference, 221.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 persons, 2 suitcases, 16.2ms\n",
      "Speed: 1.5ms preprocess, 16.2ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 1 suitcase, 11.2ms\n",
      "Speed: 1.5ms preprocess, 11.2ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 1 suitcase, 12.5ms\n",
      "Speed: 1.2ms preprocess, 12.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 2 handbags, 14.3ms\n",
      "Speed: 1.2ms preprocess, 14.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 2 handbags, 16.1ms\n",
      "Speed: 1.2ms preprocess, 16.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 persons, 1 handbag, 12.2ms\n",
      "Speed: 1.1ms preprocess, 12.2ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 1 handbag, 11.7ms\n",
      "Speed: 1.0ms preprocess, 11.7ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 1 handbag, 1 suitcase, 12.3ms\n",
      "Speed: 1.4ms preprocess, 12.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 persons, 1 backpack, 2 handbags, 1 suitcase, 12.8ms\n",
      "Speed: 1.3ms preprocess, 12.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 1 backpack, 1 suitcase, 12.5ms\n",
      "Speed: 2.0ms preprocess, 12.5ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 1 backpack, 1 handbag, 14.3ms\n",
      "Speed: 1.7ms preprocess, 14.3ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 1 backpack, 2 handbags, 12.3ms\n",
      "Speed: 1.7ms preprocess, 12.3ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 backpack, 1 handbag, 1 suitcase, 14.6ms\n",
      "Speed: 1.4ms preprocess, 14.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 1 backpack, 1 handbag, 11.9ms\n",
      "Speed: 1.4ms preprocess, 11.9ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 backpack, 1 suitcase, 11.4ms\n",
      "Speed: 1.9ms preprocess, 11.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 1 backpack, 1 handbag, 1 suitcase, 11.0ms\n",
      "Speed: 1.4ms preprocess, 11.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 suitcase, 11.4ms\n",
      "Speed: 2.0ms preprocess, 11.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 1 suitcase, 13.5ms\n",
      "Speed: 1.6ms preprocess, 13.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 1 suitcase, 13.1ms\n",
      "Speed: 1.7ms preprocess, 13.1ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 1 suitcase, 11.1ms\n",
      "Speed: 1.0ms preprocess, 11.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 1 suitcase, 14.8ms\n",
      "Speed: 1.8ms preprocess, 14.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 1 suitcase, 11.3ms\n",
      "Speed: 2.0ms preprocess, 11.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 1 suitcase, 11.8ms\n",
      "Speed: 1.2ms preprocess, 11.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 3 handbags, 1 suitcase, 10.8ms\n",
      "Speed: 1.3ms preprocess, 10.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 1 suitcase, 12.1ms\n",
      "Speed: 1.7ms preprocess, 12.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 persons, 1 suitcase, 18.7ms\n",
      "Speed: 2.9ms preprocess, 18.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 1 suitcase, 13.6ms\n",
      "Speed: 1.6ms preprocess, 13.6ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 2 suitcases, 11.9ms\n",
      "Speed: 1.6ms preprocess, 11.9ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 backpack, 2 suitcases, 13.0ms\n",
      "Speed: 2.1ms preprocess, 13.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 backpack, 1 handbag, 1 suitcase, 16.3ms\n",
      "Speed: 1.9ms preprocess, 16.3ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 backpack, 1 handbag, 1 suitcase, 14.8ms\n",
      "Speed: 2.0ms preprocess, 14.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 1 backpack, 1 handbag, 1 suitcase, 11.8ms\n",
      "Speed: 1.7ms preprocess, 11.8ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 backpack, 1 handbag, 1 suitcase, 11.5ms\n",
      "Speed: 1.8ms preprocess, 11.5ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 1 backpack, 1 handbag, 1 suitcase, 13.9ms\n",
      "Speed: 2.8ms preprocess, 13.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 1 backpack, 1 handbag, 1 suitcase, 12.0ms\n",
      "Speed: 2.1ms preprocess, 12.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 1 backpack, 1 handbag, 1 suitcase, 11.5ms\n",
      "Speed: 2.3ms preprocess, 11.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 backpack, 1 handbag, 1 suitcase, 11.0ms\n",
      "Speed: 2.3ms preprocess, 11.0ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 1 backpack, 1 handbag, 1 suitcase, 23.2ms\n",
      "Speed: 2.4ms preprocess, 23.2ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 backpack, 1 handbag, 1 suitcase, 11.4ms\n",
      "Speed: 1.9ms preprocess, 11.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 backpack, 1 handbag, 1 suitcase, 15.2ms\n",
      "Speed: 1.5ms preprocess, 15.2ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 handbag, 1 suitcase, 12.8ms\n",
      "Speed: 2.2ms preprocess, 12.8ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 2 handbags, 2 suitcases, 14.8ms\n",
      "Speed: 2.3ms preprocess, 14.8ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 2 handbags, 1 suitcase, 12.7ms\n",
      "Speed: 2.0ms preprocess, 12.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 2 handbags, 1 suitcase, 10.8ms\n",
      "Speed: 5.3ms preprocess, 10.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 2 handbags, 1 suitcase, 11.2ms\n",
      "Speed: 1.9ms preprocess, 11.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 handbag, 1 suitcase, 15.0ms\n",
      "Speed: 2.1ms preprocess, 15.0ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 3 handbags, 1 suitcase, 10.7ms\n",
      "Speed: 1.6ms preprocess, 10.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 1 handbag, 1 suitcase, 11.9ms\n",
      "Speed: 2.3ms preprocess, 11.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 handbag, 1 suitcase, 16.1ms\n",
      "Speed: 1.7ms preprocess, 16.1ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 1 suitcase, 12.0ms\n",
      "Speed: 1.6ms preprocess, 12.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 handbag, 1 suitcase, 11.9ms\n",
      "Speed: 2.4ms preprocess, 11.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 handbag, 1 suitcase, 19.9ms\n",
      "Speed: 2.9ms preprocess, 19.9ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 handbag, 1 suitcase, 12.1ms\n",
      "Speed: 2.3ms preprocess, 12.1ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 1 handbag, 1 suitcase, 13.7ms\n",
      "Speed: 2.4ms preprocess, 13.7ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 2 handbags, 1 suitcase, 28.8ms\n",
      "Speed: 2.3ms preprocess, 28.8ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 handbag, 2 suitcases, 21.8ms\n",
      "Speed: 4.0ms preprocess, 21.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 2 handbags, 1 suitcase, 11.9ms\n",
      "Speed: 1.8ms preprocess, 11.9ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 2 handbags, 1 suitcase, 13.2ms\n",
      "Speed: 2.7ms preprocess, 13.2ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 2 handbags, 1 suitcase, 18.5ms\n",
      "Speed: 2.4ms preprocess, 18.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 1 backpack, 2 handbags, 1 suitcase, 18.5ms\n",
      "Speed: 2.9ms preprocess, 18.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 2 backpacks, 2 handbags, 1 suitcase, 16.1ms\n",
      "Speed: 2.6ms preprocess, 16.1ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 2 handbags, 2 suitcases, 13.1ms\n",
      "Speed: 1.7ms preprocess, 13.1ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 2 handbags, 1 suitcase, 12.3ms\n",
      "Speed: 2.0ms preprocess, 12.3ms inference, 4.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 2 handbags, 1 suitcase, 13.4ms\n",
      "Speed: 1.8ms preprocess, 13.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 backpack, 1 handbag, 1 suitcase, 14.0ms\n",
      "Speed: 1.7ms preprocess, 14.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 backpack, 1 handbag, 1 suitcase, 13.4ms\n",
      "Speed: 1.7ms preprocess, 13.4ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 2 backpacks, 1 handbag, 1 suitcase, 12.1ms\n",
      "Speed: 2.2ms preprocess, 12.1ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 1 backpack, 1 handbag, 1 suitcase, 14.7ms\n",
      "Speed: 2.1ms preprocess, 14.7ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 2 handbags, 1 suitcase, 15.8ms\n",
      "Speed: 2.0ms preprocess, 15.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 backpack, 1 handbag, 1 suitcase, 15.5ms\n",
      "Speed: 3.1ms preprocess, 15.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 persons, 2 handbags, 1 suitcase, 12.3ms\n",
      "Speed: 2.3ms preprocess, 12.3ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 3 handbags, 1 suitcase, 14.3ms\n",
      "Speed: 3.7ms preprocess, 14.3ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 1 backpack, 2 handbags, 1 suitcase, 11.6ms\n",
      "Speed: 2.2ms preprocess, 11.6ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 1 backpack, 3 handbags, 1 suitcase, 11.7ms\n",
      "Speed: 2.0ms preprocess, 11.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 1 backpack, 2 handbags, 1 suitcase, 13.7ms\n",
      "Speed: 2.6ms preprocess, 13.7ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 1 backpack, 2 handbags, 1 suitcase, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 3 handbags, 1 suitcase, 11.9ms\n",
      "Speed: 1.8ms preprocess, 11.9ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 1 backpack, 1 handbag, 1 suitcase, 17.3ms\n",
      "Speed: 1.5ms preprocess, 17.3ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 2 handbags, 1 suitcase, 12.1ms\n",
      "Speed: 1.9ms preprocess, 12.1ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 2 handbags, 1 suitcase, 17.4ms\n",
      "Speed: 2.5ms preprocess, 17.4ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 2 handbags, 1 suitcase, 19.1ms\n",
      "Speed: 2.2ms preprocess, 19.1ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 1 handbag, 13.4ms\n",
      "Speed: 2.2ms preprocess, 13.4ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 3 handbags, 1 suitcase, 11.9ms\n",
      "Speed: 1.2ms preprocess, 11.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 2 handbags, 1 suitcase, 12.4ms\n",
      "Speed: 1.5ms preprocess, 12.4ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 handbag, 2 suitcases, 10.1ms\n",
      "Speed: 1.1ms preprocess, 10.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 backpack, 1 handbag, 2 suitcases, 15.1ms\n",
      "Speed: 1.6ms preprocess, 15.1ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 backpack, 2 handbags, 2 suitcases, 12.1ms\n",
      "Speed: 2.0ms preprocess, 12.1ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 2 backpacks, 2 handbags, 2 suitcases, 10.5ms\n",
      "Speed: 2.0ms preprocess, 10.5ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 backpack, 1 handbag, 2 suitcases, 12.8ms\n",
      "Speed: 2.2ms preprocess, 12.8ms inference, 4.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 2 backpacks, 1 handbag, 2 suitcases, 13.2ms\n",
      "Speed: 2.5ms preprocess, 13.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 2 backpacks, 2 handbags, 1 suitcase, 12.7ms\n",
      "Speed: 2.3ms preprocess, 12.7ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 2 backpacks, 1 handbag, 2 suitcases, 12.7ms\n",
      "Speed: 3.6ms preprocess, 12.7ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 2 backpacks, 1 handbag, 1 suitcase, 13.8ms\n",
      "Speed: 2.8ms preprocess, 13.8ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 backpack, 1 handbag, 1 suitcase, 12.2ms\n",
      "Speed: 1.8ms preprocess, 12.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 backpack, 1 handbag, 2 suitcases, 12.2ms\n",
      "Speed: 2.2ms preprocess, 12.2ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 backpack, 1 handbag, 2 suitcases, 14.1ms\n",
      "Speed: 2.3ms preprocess, 14.1ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 1 backpack, 3 handbags, 1 suitcase, 15.0ms\n",
      "Speed: 2.6ms preprocess, 15.0ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 persons, 1 backpack, 1 handbag, 1 suitcase, 11.4ms\n",
      "Speed: 1.8ms preprocess, 11.4ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 persons, 2 handbags, 1 suitcase, 12.6ms\n",
      "Speed: 2.7ms preprocess, 12.6ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 persons, 3 handbags, 1 suitcase, 12.8ms\n",
      "Speed: 2.1ms preprocess, 12.8ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 1 handbag, 1 suitcase, 12.2ms\n",
      "Speed: 4.1ms preprocess, 12.2ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 persons, 2 handbags, 3 suitcases, 15.7ms\n",
      "Speed: 2.6ms preprocess, 15.7ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 2 handbags, 2 suitcases, 12.5ms\n",
      "Speed: 1.6ms preprocess, 12.5ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 2 handbags, 1 suitcase, 11.9ms\n",
      "Speed: 1.3ms preprocess, 11.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 3 handbags, 1 suitcase, 13.0ms\n",
      "Speed: 2.2ms preprocess, 13.0ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 1 backpack, 4 handbags, 1 skateboard, 11.0ms\n",
      "Speed: 1.1ms preprocess, 11.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 3 handbags, 1 suitcase, 10.9ms\n",
      "Speed: 1.5ms preprocess, 10.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 5 handbags, 1 suitcase, 11.5ms\n",
      "Speed: 1.0ms preprocess, 11.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 5 handbags, 2 suitcases, 16.8ms\n",
      "Speed: 1.5ms preprocess, 16.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 6 handbags, 1 suitcase, 12.2ms\n",
      "Speed: 2.0ms preprocess, 12.2ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 5 handbags, 1 suitcase, 12.5ms\n",
      "Speed: 1.8ms preprocess, 12.5ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 5 handbags, 1 suitcase, 12.2ms\n",
      "Speed: 1.2ms preprocess, 12.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 3 handbags, 1 suitcase, 11.9ms\n",
      "Speed: 1.5ms preprocess, 11.9ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 4 handbags, 1 suitcase, 19.8ms\n",
      "Speed: 1.9ms preprocess, 19.8ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 3 handbags, 1 suitcase, 12.4ms\n",
      "Speed: 2.0ms preprocess, 12.4ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 4 handbags, 1 suitcase, 15.5ms\n",
      "Speed: 2.1ms preprocess, 15.5ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 3 handbags, 1 suitcase, 10.9ms\n",
      "Speed: 2.2ms preprocess, 10.9ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 4 handbags, 1 suitcase, 12.2ms\n",
      "Speed: 1.9ms preprocess, 12.2ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 5 handbags, 1 suitcase, 18.1ms\n",
      "Speed: 2.0ms preprocess, 18.1ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 2 handbags, 1 suitcase, 13.2ms\n",
      "Speed: 2.6ms preprocess, 13.2ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 handbag, 1 suitcase, 12.8ms\n",
      "Speed: 2.2ms preprocess, 12.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 3 handbags, 1 suitcase, 10.7ms\n",
      "Speed: 1.8ms preprocess, 10.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 2 handbags, 1 suitcase, 12.5ms\n",
      "Speed: 2.3ms preprocess, 12.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 2 handbags, 1 suitcase, 13.9ms\n",
      "Speed: 1.9ms preprocess, 13.9ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 3 handbags, 1 suitcase, 12.0ms\n",
      "Speed: 2.1ms preprocess, 12.0ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 4 handbags, 14.3ms\n",
      "Speed: 2.0ms preprocess, 14.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 5 handbags, 1 suitcase, 11.3ms\n",
      "Speed: 1.6ms preprocess, 11.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 3 handbags, 1 suitcase, 10.6ms\n",
      "Speed: 1.8ms preprocess, 10.6ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 2 handbags, 1 suitcase, 11.9ms\n",
      "Speed: 1.8ms preprocess, 11.9ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 4 handbags, 1 suitcase, 12.7ms\n",
      "Speed: 2.0ms preprocess, 12.7ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 4 handbags, 1 suitcase, 12.6ms\n",
      "Speed: 2.5ms preprocess, 12.6ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 3 handbags, 1 suitcase, 15.0ms\n",
      "Speed: 4.7ms preprocess, 15.0ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 2 handbags, 2 suitcases, 13.4ms\n",
      "Speed: 2.0ms preprocess, 13.4ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 2 handbags, 1 suitcase, 13.1ms\n",
      "Speed: 2.8ms preprocess, 13.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 1 handbag, 1 suitcase, 14.8ms\n",
      "Speed: 2.5ms preprocess, 14.8ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 2 handbags, 1 suitcase, 12.6ms\n",
      "Speed: 2.7ms preprocess, 12.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 3 handbags, 1 suitcase, 12.0ms\n",
      "Speed: 1.6ms preprocess, 12.0ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 handbag, 1 suitcase, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 2 handbags, 1 suitcase, 24.7ms\n",
      "Speed: 2.1ms preprocess, 24.7ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 handbag, 1 suitcase, 11.8ms\n",
      "Speed: 2.2ms preprocess, 11.8ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 3 handbags, 2 suitcases, 12.4ms\n",
      "Speed: 5.0ms preprocess, 12.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 4 handbags, 1 suitcase, 13.3ms\n",
      "Speed: 2.1ms preprocess, 13.3ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 3 handbags, 2 suitcases, 13.5ms\n",
      "Speed: 2.3ms preprocess, 13.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 3 handbags, 1 suitcase, 16.1ms\n",
      "Speed: 2.7ms preprocess, 16.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 3 handbags, 1 suitcase, 11.8ms\n",
      "Speed: 5.4ms preprocess, 11.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 3 handbags, 1 suitcase, 17.1ms\n",
      "Speed: 2.3ms preprocess, 17.1ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 3 handbags, 1 suitcase, 11.2ms\n",
      "Speed: 1.5ms preprocess, 11.2ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 3 handbags, 1 suitcase, 12.5ms\n",
      "Speed: 1.8ms preprocess, 12.5ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 3 handbags, 1 suitcase, 12.1ms\n",
      "Speed: 4.0ms preprocess, 12.1ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 3 handbags, 1 suitcase, 16.5ms\n",
      "Speed: 2.1ms preprocess, 16.5ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 3 handbags, 2 suitcases, 12.7ms\n",
      "Speed: 1.9ms preprocess, 12.7ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 3 handbags, 1 suitcase, 14.8ms\n",
      "Speed: 2.5ms preprocess, 14.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 3 handbags, 1 suitcase, 11.0ms\n",
      "Speed: 1.9ms preprocess, 11.0ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 3 handbags, 1 suitcase, 12.5ms\n",
      "Speed: 2.2ms preprocess, 12.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 2 handbags, 1 suitcase, 12.0ms\n",
      "Speed: 2.2ms preprocess, 12.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 handbag, 2 suitcases, 12.9ms\n",
      "Speed: 3.9ms preprocess, 12.9ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 handbag, 2 suitcases, 10.4ms\n",
      "Speed: 2.0ms preprocess, 10.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 handbag, 1 suitcase, 15.0ms\n",
      "Speed: 2.4ms preprocess, 15.0ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 1 handbag, 2 suitcases, 14.3ms\n",
      "Speed: 1.9ms preprocess, 14.3ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 2 handbags, 1 suitcase, 22.7ms\n",
      "Speed: 2.3ms preprocess, 22.7ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 2 handbags, 1 suitcase, 12.4ms\n",
      "Speed: 2.3ms preprocess, 12.4ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 handbag, 1 suitcase, 20.0ms\n",
      "Speed: 1.8ms preprocess, 20.0ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 2 handbags, 1 suitcase, 11.0ms\n",
      "Speed: 4.8ms preprocess, 11.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 2 handbags, 1 suitcase, 13.4ms\n",
      "Speed: 2.0ms preprocess, 13.4ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 handbag, 1 suitcase, 15.3ms\n",
      "Speed: 2.3ms preprocess, 15.3ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 handbag, 1 suitcase, 13.1ms\n",
      "Speed: 2.0ms preprocess, 13.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 2 handbags, 1 suitcase, 12.4ms\n",
      "Speed: 1.8ms preprocess, 12.4ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 handbag, 1 suitcase, 13.9ms\n",
      "Speed: 5.4ms preprocess, 13.9ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 2 handbags, 1 suitcase, 12.2ms\n",
      "Speed: 2.3ms preprocess, 12.2ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 3 handbags, 1 suitcase, 13.6ms\n",
      "Speed: 2.2ms preprocess, 13.6ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 2 handbags, 1 suitcase, 12.0ms\n",
      "Speed: 2.6ms preprocess, 12.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 2 handbags, 1 suitcase, 14.6ms\n",
      "Speed: 2.2ms preprocess, 14.6ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 handbag, 1 suitcase, 11.8ms\n",
      "Speed: 2.0ms preprocess, 11.8ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 handbag, 1 suitcase, 20.8ms\n",
      "Speed: 1.5ms preprocess, 20.8ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 handbag, 1 suitcase, 13.5ms\n",
      "Speed: 1.3ms preprocess, 13.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 handbag, 1 suitcase, 30.3ms\n",
      "Speed: 1.5ms preprocess, 30.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 handbag, 1 suitcase, 13.4ms\n",
      "Speed: 2.0ms preprocess, 13.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 handbag, 1 suitcase, 20.4ms\n",
      "Speed: 2.3ms preprocess, 20.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 handbag, 1 suitcase, 26.9ms\n",
      "Speed: 1.8ms preprocess, 26.9ms inference, 4.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 handbag, 1 suitcase, 30.9ms\n",
      "Speed: 2.1ms preprocess, 30.9ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 handbag, 1 suitcase, 12.5ms\n",
      "Speed: 1.7ms preprocess, 12.5ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 suitcase, 12.4ms\n",
      "Speed: 1.8ms preprocess, 12.4ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 suitcase, 15.6ms\n",
      "Speed: 2.0ms preprocess, 15.6ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 backpack, 1 suitcase, 17.8ms\n",
      "Speed: 1.8ms preprocess, 17.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 backpack, 1 handbag, 1 suitcase, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 backpack, 1 suitcase, 11.3ms\n",
      "Speed: 2.7ms preprocess, 11.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 backpack, 1 handbag, 1 suitcase, 11.7ms\n",
      "Speed: 2.0ms preprocess, 11.7ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 backpack, 1 handbag, 1 suitcase, 10.8ms\n",
      "Speed: 2.1ms preprocess, 10.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 2 backpacks, 1 handbag, 1 suitcase, 12.1ms\n",
      "Speed: 2.0ms preprocess, 12.1ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 1 backpack, 2 handbags, 1 suitcase, 12.6ms\n",
      "Speed: 2.0ms preprocess, 12.6ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 backpack, 1 handbag, 1 suitcase, 18.1ms\n",
      "Speed: 5.0ms preprocess, 18.1ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 backpack, 2 handbags, 1 suitcase, 12.5ms\n",
      "Speed: 2.3ms preprocess, 12.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 backpack, 3 handbags, 1 suitcase, 10.3ms\n",
      "Speed: 1.8ms preprocess, 10.3ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 backpack, 3 handbags, 1 suitcase, 12.3ms\n",
      "Speed: 2.2ms preprocess, 12.3ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 backpack, 2 handbags, 1 suitcase, 15.4ms\n",
      "Speed: 2.5ms preprocess, 15.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 backpack, 3 handbags, 1 suitcase, 13.2ms\n",
      "Speed: 2.3ms preprocess, 13.2ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 backpack, 3 handbags, 14.4ms\n",
      "Speed: 1.9ms preprocess, 14.4ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 2 backpacks, 2 handbags, 1 suitcase, 10.8ms\n",
      "Speed: 1.8ms preprocess, 10.8ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 backpack, 1 handbag, 1 suitcase, 18.1ms\n",
      "Speed: 2.1ms preprocess, 18.1ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 backpack, 3 handbags, 1 suitcase, 11.8ms\n",
      "Speed: 1.9ms preprocess, 11.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 backpack, 3 handbags, 1 suitcase, 13.9ms\n",
      "Speed: 3.4ms preprocess, 13.9ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 backpack, 4 handbags, 1 suitcase, 11.6ms\n",
      "Speed: 2.0ms preprocess, 11.6ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 backpack, 3 handbags, 1 suitcase, 11.3ms\n",
      "Speed: 2.0ms preprocess, 11.3ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 backpack, 2 handbags, 2 suitcases, 13.0ms\n",
      "Speed: 2.7ms preprocess, 13.0ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 backpack, 3 handbags, 1 suitcase, 12.9ms\n",
      "Speed: 2.0ms preprocess, 12.9ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 backpack, 2 handbags, 11.5ms\n",
      "Speed: 2.2ms preprocess, 11.5ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 backpack, 3 handbags, 15.4ms\n",
      "Speed: 2.1ms preprocess, 15.4ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 1 backpack, 2 handbags, 1 suitcase, 10.8ms\n",
      "Speed: 1.9ms preprocess, 10.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 1 backpack, 2 handbags, 1 suitcase, 12.3ms\n",
      "Speed: 2.0ms preprocess, 12.3ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 backpack, 2 handbags, 12.8ms\n",
      "Speed: 1.8ms preprocess, 12.8ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 backpack, 4 handbags, 1 suitcase, 12.5ms\n",
      "Speed: 1.8ms preprocess, 12.5ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 backpack, 1 umbrella, 4 handbags, 10.9ms\n",
      "Speed: 1.6ms preprocess, 10.9ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 backpack, 1 umbrella, 2 handbags, 16.7ms\n",
      "Speed: 2.5ms preprocess, 16.7ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 backpack, 2 handbags, 12.6ms\n",
      "Speed: 2.3ms preprocess, 12.6ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 backpack, 2 handbags, 1 suitcase, 11.8ms\n",
      "Speed: 2.4ms preprocess, 11.8ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 backpack, 3 handbags, 2 suitcases, 13.4ms\n",
      "Speed: 3.2ms preprocess, 13.4ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 backpack, 2 handbags, 1 suitcase, 13.9ms\n",
      "Speed: 2.0ms preprocess, 13.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 backpack, 3 handbags, 2 suitcases, 14.5ms\n",
      "Speed: 1.8ms preprocess, 14.5ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 backpack, 3 handbags, 1 suitcase, 13.2ms\n",
      "Speed: 2.5ms preprocess, 13.2ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 backpack, 2 handbags, 16.2ms\n",
      "Speed: 3.0ms preprocess, 16.2ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 backpack, 1 handbag, 1 suitcase, 1 skateboard, 16.7ms\n",
      "Speed: 3.5ms preprocess, 16.7ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 2 backpacks, 1 handbag, 1 suitcase, 11.1ms\n",
      "Speed: 1.9ms preprocess, 11.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 2 backpacks, 6 handbags, 2 suitcases, 12.0ms\n",
      "Speed: 2.3ms preprocess, 12.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 2 backpacks, 5 handbags, 1 suitcase, 10.3ms\n",
      "Speed: 1.5ms preprocess, 10.3ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 2 backpacks, 3 handbags, 1 suitcase, 13.6ms\n",
      "Speed: 3.0ms preprocess, 13.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 2 backpacks, 4 handbags, 1 suitcase, 11.9ms\n",
      "Speed: 2.3ms preprocess, 11.9ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 2 backpacks, 4 handbags, 1 suitcase, 11.4ms\n",
      "Speed: 2.2ms preprocess, 11.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 backpack, 4 handbags, 1 suitcase, 10.4ms\n",
      "Speed: 1.6ms preprocess, 10.4ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 backpack, 1 handbag, 1 suitcase, 12.4ms\n",
      "Speed: 1.9ms preprocess, 12.4ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 backpack, 1 handbag, 1 suitcase, 10.9ms\n",
      "Speed: 1.9ms preprocess, 10.9ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 backpack, 2 handbags, 1 suitcase, 10.4ms\n",
      "Speed: 2.2ms preprocess, 10.4ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 backpack, 2 handbags, 1 suitcase, 16.6ms\n",
      "Speed: 2.6ms preprocess, 16.6ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 backpack, 1 handbag, 1 suitcase, 12.1ms\n",
      "Speed: 1.8ms preprocess, 12.1ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 backpack, 2 handbags, 1 suitcase, 11.5ms\n",
      "Speed: 1.9ms preprocess, 11.5ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 backpack, 1 handbag, 1 suitcase, 11.8ms\n",
      "Speed: 2.0ms preprocess, 11.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 1 backpack, 2 handbags, 1 suitcase, 11.4ms\n",
      "Speed: 2.3ms preprocess, 11.4ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 backpack, 1 handbag, 1 suitcase, 10.7ms\n",
      "Speed: 2.0ms preprocess, 10.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 backpack, 4 handbags, 11.5ms\n",
      "Speed: 2.1ms preprocess, 11.5ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 backpack, 3 handbags, 11.4ms\n",
      "Speed: 2.0ms preprocess, 11.4ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 backpack, 5 handbags, 14.1ms\n",
      "Speed: 2.3ms preprocess, 14.1ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 backpack, 5 handbags, 11.8ms\n",
      "Speed: 1.1ms preprocess, 11.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 backpack, 4 handbags, 10.5ms\n",
      "Speed: 1.2ms preprocess, 10.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 backpack, 6 handbags, 13.4ms\n",
      "Speed: 1.2ms preprocess, 13.4ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "while video_cap.isOpened():\n",
    "    ret, frame = video_cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    start_time = datetime.datetime.now()\n",
    "\n",
    "    # -----------------------------\n",
    "    # 7a Detect persons with YOLOv8\n",
    "    # -----------------------------\n",
    "    results = yolo_model(frame)[0]\n",
    "    detections = []\n",
    "    embeddings = []\n",
    "\n",
    "    for data in results.boxes.data.tolist():\n",
    "        confidence = data[4]\n",
    "        if float(confidence) >= CONFIDENCE_THRESHOLD:\n",
    "            xmin, ymin, xmax, ymax = map(int, data[:4])\n",
    "            class_id = int(data[5])\n",
    "\n",
    "            if class_id == 0:  # only track persons\n",
    "                bbox = [xmin, ymin, xmax - xmin, ymax - ymin]  # DeepSORT format\n",
    "                detections.append([bbox, confidence, class_id])\n",
    "\n",
    "                # -----------------------------\n",
    "                # 7b Extract OSNet embedding for re-ID\n",
    "                # -----------------------------\n",
    "                person_img = frame[ymin:ymax, xmin:xmax]\n",
    "                if person_img.size > 0:\n",
    "                    person_img = transform(person_img).unsqueeze(0)\n",
    "                    with torch.no_grad():\n",
    "                        embedding = osnet_model(person_img)\n",
    "                    embeddings.append(embedding.squeeze(0).cpu().numpy())\n",
    "\n",
    "    # -----------------------------\n",
    "    # 7c Update DeepSORT tracker\n",
    "    # -----------------------------\n",
    "    tracks = tracker.update_tracks(detections, frame=frame)\n",
    "\n",
    "    # -----------------------------\n",
    "    # 7d Apply re-identification logic and suspicious detection\n",
    "    # -----------------------------\n",
    "    for i, track in enumerate(tracks):\n",
    "        if not track.is_confirmed() or track.det_class != 0:\n",
    "            continue\n",
    "\n",
    "        track_id = track.track_id\n",
    "        ltrb = track.to_ltrb()\n",
    "        xmin, ymin, xmax, ymax = map(int, ltrb)\n",
    "        x_center = (xmin + xmax) // 2\n",
    "        y_center = (ymin + ymax) // 2\n",
    "\n",
    "        # Update position history\n",
    "        if track_id not in track_positions:\n",
    "            track_positions[track_id] = []\n",
    "        track_positions[track_id].append((x_center, y_center))\n",
    "        if len(track_positions[track_id]) > SUSPICIOUS_TIME_FRAMES:\n",
    "            track_positions[track_id].pop(0)\n",
    "\n",
    "        if i < len(embeddings):\n",
    "            current_embedding = embeddings[i]\n",
    "\n",
    "            if track_id not in track_embedding_history:\n",
    "                # Check delay for re-identification\n",
    "                if track_id in last_seen and (video_cap.get(cv2.CAP_PROP_POS_FRAMES) - last_seen[track_id]) < REIDENTIFY_DELAY_FRAMES:\n",
    "                    continue\n",
    "\n",
    "                # Compare with past embeddings\n",
    "                for past_id, past_embedding in track_embedding_history.items():\n",
    "                    distance = cosine(current_embedding, past_embedding)\n",
    "                    if distance < 0.25:  # threshold\n",
    "                        track_id = past_id\n",
    "                        break\n",
    "\n",
    "            # Update history\n",
    "            track_embedding_history[track_id] = current_embedding\n",
    "            last_seen[track_id] = video_cap.get(cv2.CAP_PROP_POS_FRAMES)\n",
    "\n",
    "        # -----------------------------\n",
    "        # 7e Detect suspicious behavior\n",
    "        # -----------------------------\n",
    "        suspicious = is_suspicious(track_positions[track_id])\n",
    "\n",
    "        # Box color\n",
    "        color_box = RED if suspicious else GREEN\n",
    "\n",
    "        # Draw bounding box and ID\n",
    "        cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), color_box, 2)\n",
    "        cv2.putText(frame, f\"ID: {track_id}\", (xmin, ymin - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, WHITE, 2)\n",
    "        if suspicious:\n",
    "            cv2.putText(frame, \"SUSPICIOUS\", (xmin, ymax + 20),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, RED, 2)\n",
    "\n",
    "    # -----------------------------\n",
    "    # 7f Display FPS\n",
    "    # -----------------------------\n",
    "    end_time = datetime.datetime.now()\n",
    "    fps_text = f\"FPS: {1 / (end_time - start_time).total_seconds():.2f}\"\n",
    "    cv2.putText(frame, fps_text, (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "    # -----------------------------\n",
    "    # 7g Show and save video\n",
    "    # -----------------------------\n",
    "    cv2.imshow(\"Person Detection & Tracking (OSNet + Suspicious)\", frame)\n",
    "    out.write(frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# -----------------------------\n",
    "# 8 Release resources\n",
    "# -----------------------------\n",
    "video_cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040c1922",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu-neuro",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
